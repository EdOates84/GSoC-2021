{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalNotebook'21.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFOBYWEtDnSL",
        "outputId": "5af9687d-f615-40b8-d477-7ed1cd78aecc"
      },
      "source": [
        "pip install wikipedia"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp37-none-any.whl size=11697 sha256=ed1bb12d180b394b52a2da224272b765efd9e63a8363e597001960db1c049744\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-sVdietD30o",
        "outputId": "c97a79f2-c04f-4692-901d-0b9dd4f22e49"
      },
      "source": [
        "pip install face_recognition"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.19.5)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K     |████████████████████████████████| 100.2MB 89kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566184 sha256=8f49b84cb523ef6d50b9347a7e126f31027b785e58181bebec9dda737df735b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BfK19SnDwWF"
      },
      "source": [
        "import os\n",
        "import time as tm\n",
        "import cv2\n",
        "import pickle\n",
        "import dlib\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from collections import Counter\n",
        "\n",
        "import wikipedia as wiki\n",
        "import pandas as pd\n",
        "\n",
        "GENRE_TYPES = 'Talk-Show|News'\n",
        "TITLE_TYPE = 'tvSeries'\n",
        "PROFESSION_TYPES =  [' host',' presenter',' journalist',' news anchor','correspondent',' anchor']\n",
        "SPLIT_TYPES = [' is a ',' is an ',' was a ',' was an ',' is the ']\n",
        "# IMDB_PROFESSION_TYPES = 'actor|actress|miscellaneous|Null'\n",
        "NONE = 'Not Found'\n",
        "PAGE_ERROR = 'does not match any pages'\n",
        "DISAMBIGUATION_ERROR = 'disambiguation'\n",
        "NETWORK_TYPES = ['FOX','ABC','CBS','NBC','CNN','United Paramount Net','Warner Bros.','Pure Independent','PBS','Pax TV','Telemundo']\n",
        "REGION_TYPE = 'US'\n",
        "SHOW_NOT_IN_IMDB = 'Show is not present in IMDb'\n",
        "NAME_BASICS = \"/content/drive/MyDrive/Datasets/IMDB_Datasets/name.basics.tsv/data.tsv\"\n",
        "TITLE_BASICS = \"/content/drive/MyDrive/Datasets/IMDB_Datasets/title.basics.tsv/data.tsv\"\n",
        "TITLE_PRINCIPALS = \"/content/drive/MyDrive/Datasets/IMDB_Datasets/title.principals.tsv.gz\"\n",
        "TITLE_AKAS = \"/content/drive/MyDrive/Datasets/IMDB_Datasets/title.akas.tsv/data.tsv\"\n",
        "TITLE_CREW = \"/content/drive/MyDrive/Datasets/IMDB_Datasets/title.crew.tsv/data.tsv\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePWrWc97yrvO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "39755357-984e-4b89-cc5a-8abda11406cc"
      },
      "source": [
        "def loadPickle(filename):\n",
        "    infile = open(filename,'rb')\n",
        "    return pickle.load(infile, encoding='latin1')\n",
        "\n",
        "def storePickle(filename, data):\n",
        "    file = open(filename,'wb')\n",
        "    pickle.dump(data, file)\n",
        "    file.close()\n",
        "\n",
        "\n",
        "def sec2HMS(seconds):\n",
        "    return tm.strftime('%H:%M:%S', tm.gmtime(seconds))\n",
        "\n",
        "def HMS2sec(time_str):\n",
        "    h, m, s = time_str.split(':')\n",
        "    return int(h) * 3600 + int(m) * 60 + int(s)\n",
        "\n",
        "celebs, celeb_encodings = loadPickle('/content/drive/MyDrive/Datasets/Show_Segmentation_2020/final_celeb_detection/final_pickles/anchors-with-TV-encodings.pickle')\n",
        "celeb_encodings = np.array([np.array(x) for x in celeb_encodings])\n",
        "\n",
        "\n",
        "# Populating KNN space with labelled encodings\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(len(celeb_encodings)): #prepare dataset\n",
        "    for celeb_encoding in celeb_encodings[i]:\n",
        "        X.append(celeb_encoding)\n",
        "        Y.append(celebs[i])\n",
        "        \n",
        "neigh = KNeighborsClassifier(n_neighbors=30)\n",
        "neigh.fit(X, Y)\n",
        "\n",
        "\n",
        "def encoding2name(f_encodings):\n",
        "        return neigh.predict(f_encodings)\n",
        "\n",
        "\n",
        "vid_path = '/content/drive/MyDrive/Datasets/2006-01-02_0000_US_00001057_V11_M2_VHS10_H4_JA.mp4'\n",
        "\n",
        "\n",
        "#TODO - take input from the user - next cell as well\n",
        "output_path = '/content/drive/My Drive/Datasets/newtest'\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "\n",
        "\n",
        "class Show:\n",
        "    def __init__(self, hosts, start_time, end_time):\n",
        "        self.hosts = hosts\n",
        "        self.start_time = start_time\n",
        "        self.end_time = end_time\n",
        "\n",
        "\n",
        "def getEncodings(vid_path, skip_seconds=1):\n",
        "    \"\"\" Returns 129D encodings for each face present in the video\n",
        "        First 128 are the face encodings and the last value is the time.\n",
        "        \n",
        "       :param interval: (in seconds) frame interval to skip and look for faces\n",
        "       :param model: 'hog' is less accurate but faster compared to 'cnn'\n",
        "       :param store: if True, stores the faces in directory specified by dirName \"\"\"\n",
        "    vidcap = cv2.VideoCapture(vid_path)\n",
        "    cv2.VideoCapture \n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps+1)*skip_seconds #no. of frames to skip\n",
        "    print(\"FPS of the video: {}\".format(fps))\n",
        "    allEncodings = [] #Dict containing path, box and encoding\n",
        "    n_frame = 0 # no. of frame being processed\n",
        "\n",
        "    success, frame = vidcap.read()\n",
        "    \n",
        "    while success:\n",
        "        rgb = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "        bboxes = face_recognition.face_locations(rgb,model='hog')\n",
        "        encodings = face_recognition.face_encodings(rgb,bboxes)\n",
        "        for i,bbox in enumerate(bboxes): #for each found face in the frame\n",
        "            top,right,bottom,left = bbox[0],bbox[1],bbox[2],bbox[3]\n",
        "            face_img = frame[top:bottom, left:right]\n",
        "            \n",
        "            d = {'time': (n_frame/fps), 'loc': bbox, 'encoding':encodings[i]}\n",
        "            # print(d)\n",
        "            allEncodings.append(d)\n",
        "            \n",
        "        n_frame += interval            \n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES,n_frame)\n",
        "        success, frame = vidcap.read()\n",
        "    \n",
        "    cv2.destroyAllWindows()\n",
        "    vidcap.release()\n",
        "    return allEncodings\n",
        "\n",
        "\n",
        "def clusterFaces(allEncodings, n_jobs=-1, max_clusters = 100):\n",
        "    \"\"\"face_times: a Tuple list with x[0] as face_classes, and x[1] as list of their\n",
        "    times of occurence in the video.\n",
        "    \n",
        "       face_encodings: dict mapping face_class to list of encodings of all occurences \n",
        "       of that face in the video\"\"\"\n",
        "    encodings = [d['encoding'] for d in allEncodings]\n",
        "    times = [d['time'] for d in allEncodings]\n",
        "    # loc = [d['loc'] for d in allEncodings]\n",
        "    # file_names = [d['time'] for d in allEncodings]\n",
        "    clt = DBSCAN(metric='euclidean', n_jobs=n_jobs, min_samples=5,eps=0.37)\n",
        "    clt.fit(encodings)\n",
        "    labels = clt.labels_ #-1 => too small to include in a cluster\n",
        "    \n",
        "    face_times = [] #list of tuples, first element is time of occurence, 2nd is the class of the face\n",
        "    face_encodings = {} #dict mapping from a face_classs to all encodings of that faces (from the images found in the video)\n",
        "    # face_index = []\n",
        "\n",
        "    for i,label in enumerate(labels):\n",
        "        encoding = encodings[i]\n",
        "        time = times[i]\n",
        "        if(label!=-1):\n",
        "            face_times.append((int(time), str(label)))\n",
        "            \n",
        "            if label in face_encodings:\n",
        "                face_encodings[label].append(encoding)\n",
        "            else:\n",
        "                face_encodings[label] = [encoding]\n",
        "    return face_times,face_encodings\n",
        "\n",
        "\n",
        "def addEmptyFaces(faces, skip_seconds):\n",
        "    \"\"\"Modifies faces dict to include timestamps where no faces are present\n",
        "       '-1' is the value assigned to these.\n",
        "       :skip_gap: 'interval' parameter given in file2encoding() function (in seconds)\"\"\"\n",
        "    min_time = (faces[0][0])\n",
        "    max_time = (faces[-1][0])\n",
        "    curr_time = min_time\n",
        "    faces_empty = []\n",
        "    counter = 0\n",
        "    \n",
        "    while (curr_time < max_time):\n",
        "        if((faces[counter][0]) > curr_time): #No face found at this time\n",
        "            faces_empty.append(((curr_time), '-1'))\n",
        "        else:                              #Face was already marked at this time\n",
        "            faces_empty.append(faces[counter])\n",
        "            counter+=1\n",
        "        curr_time += skip_seconds\n",
        "    return faces_empty\n",
        "\n",
        "\n",
        "def faceTrendsDuration(faces, interval = 900, overlapping = False, join_consecutive = False,n_top=10):\n",
        "    \"\"\"Trendy faces are the faces of an actor which occur the most in a given interval.\n",
        "       Video is split into *interval*s and most occuring faces in them are noted.\n",
        "       For each interval, *n_top* no. of most occuring faces are returned\n",
        "       in a dict format.\"\"\"\n",
        "    #GOTO JUMPER if change interval\n",
        "    # interval - SKIP_INTERVAL*interval time duration is taken as length of one trend_bucket\n",
        "    trending_face = faces[0][1] #First face's class\n",
        "    trendy_faces = {}\n",
        "    \n",
        "    if overlapping:\n",
        "        skip=1\n",
        "    else:\n",
        "        skip=interval\n",
        "    for x in range(0, len(faces), skip):\n",
        "        face_count = {} #Keeps count of no. of instances of each face_class\n",
        "        interval_string = sec2HMS(faces[x][0])\n",
        "        for face in faces[x:min(len(faces),x+interval)]:\n",
        "            curr_time = face[0]\n",
        "            curr_face = face[1]\n",
        "            if curr_face == '-1':\n",
        "                continue\n",
        "                \n",
        "            if curr_face in face_count:\n",
        "                face_count[curr_face] = (face_count[curr_face][0],curr_time)\n",
        "            else:\n",
        "                face_count[curr_face] = (curr_time, curr_time)\n",
        "                \n",
        "        if face_count: # if face_count is not empty\n",
        "            max_face_in_interval = sorted(list(face_count.keys()), key =(lambda key: (face_count[key][1]) - (face_count[key][0])),reverse=True)[:n_top]\n",
        "        else:\n",
        "            max_face_in_interval = ['-1']\n",
        "        if join_consecutive:\n",
        "            if(max_face_in_interval!=trending_face):\n",
        "                trending_face = max_face_in_interval\n",
        "                trendy_faces[interval_string] = trending_face\n",
        "        else:\n",
        "            trending_face = max_face_in_interval\n",
        "            trendy_faces[interval_string] = trending_face\n",
        "\n",
        "    return trendy_faces\n",
        "\n",
        "\n",
        "SKIP_SECONDS = 1 \n",
        "allEncodings = getEncodings(vid_path, SKIP_SECONDS)\n",
        "face_list, face_encodings = clusterFaces(allEncodings)\n",
        "\n",
        "\n",
        "faces_empty = addEmptyFaces(face_list, SKIP_SECONDS)\n",
        "trends = faceTrendsDuration(faces_empty)\n",
        "\n",
        "\n",
        "face_dict = {} #dict having all occurences of each face\n",
        "for x in face_list: \n",
        "    if x[1] in face_dict:\n",
        "        face_dict[x[1]].append(x[0])\n",
        "    else:\n",
        "        face_dict[x[1]] = [x[0]]\n",
        "\n",
        "\n",
        "#Getting consecutives\n",
        "cons_dict = {}\n",
        "for key,vals in trends.items():\n",
        "    key = HMS2sec(key)\n",
        "    for val in vals:\n",
        "        if val in cons_dict:\n",
        "            if (cons_dict[val][-1][-1]==prev_time):\n",
        "                cons_dict[val][-1].append(key)\n",
        "            else:\n",
        "                cons_dict[val].append([key])\n",
        "        else:\n",
        "            cons_dict[val] = [[key]]\n",
        "    prev_time = key\n",
        "\n",
        "\n",
        "face_intervals = {} #Dict containing exact timestamps of all occurences of an actor's face\n",
        "                    #in intervals specified by 'cons_dict'\n",
        "    \n",
        "for face,intervals in cons_dict.items():\n",
        "    face_intervals[face] = []\n",
        "    for times in intervals:\n",
        "        lb = min(x for x in face_dict[face] if x >= times[0]) #lower bound\n",
        "        ub = max(x for x in face_dict[face] if (x <= times[-1]+900)) #upper bound\n",
        "        face_intervals[face].append([x for x in face_dict[face] if (x>=lb and x<=ub)])\n",
        "\n",
        "\n",
        "#Converting the dict to a bunch of tuples of the form (face,each_interval)\n",
        "shows = [(face,times) for face in face_intervals.keys() for times in face_intervals[face]]\n",
        "shows = sorted(shows, key = lambda x: x[1][-1]) #Sorting face intervals by their order of ending time.\n",
        "shows = [list(x) for x in shows]\n",
        "\n",
        "\n",
        "#Removing too short\n",
        "min_len = 0.5*60 #In seconds #CHANGE\n",
        "shows = [x for x in shows if (x[1][-1] - x[1][0])>=min_len]\n",
        "#Will still be sorted by ending time\n",
        "\n",
        "\n",
        "# Removing intervals within intervals:\n",
        "show_intervals = [x[1] for x in shows]\n",
        "i = 0\n",
        "for x in range(len(shows)):\n",
        "    curr_interval = shows[i][1]\n",
        "    for x in show_intervals:\n",
        "        if(curr_interval[0]>x[0] and curr_interval[-1]<x[-1]):\n",
        "            del(shows[i])\n",
        "            i -= 1\n",
        "            break\n",
        "    i += 1   \n",
        "\n",
        "\n",
        "# Combining consecutive shows with very high overlap\n",
        "i = 0\n",
        "overlap_threshold = 0.75\n",
        "while(i<len(shows)-1):\n",
        "    diff = shows[i][1][-1] - shows[i+1][1][0]\n",
        "    #total = shows[i+1][1][-1] - shows[i][1][0]\n",
        "    short_show = min(shows[i][1][-1]-shows[i][1][0],shows[i+1][1][-1]-shows[i+1][1][0])\n",
        "    overlap = diff/short_show\n",
        "    if(overlap > overlap_threshold):\n",
        "        print('Hosts: {} & {}'.format(shows[i][0],shows[i+1][0]))\n",
        "        print('Original durations: {} to {} and {} to {}'.format(sec2HMS(shows[i][1][0]),sec2HMS(shows[i][1][-1]),sec2HMS(shows[i+1][1][0]),sec2HMS(shows[i+1][1][-1])))\n",
        "        print('Total duration: '+str(diff))\n",
        "        print('Overlap: '+str(overlap))\n",
        "        lb = min(shows[i][1][0],shows[i+1][1][0])\n",
        "        ub = max(shows[i][1][-1],shows[i+1][1][-1])\n",
        "        shows[i][0] = shows[i][0]+'&'+shows[i+1][0]\n",
        "        shows[i][1].extend(shows[i+1][1])\n",
        "        shows[i][1] = sorted(shows[i][1])\n",
        "        print('Merging show {} from {} to {}'.format(shows[i+1][0],shows[i+1][1][0],shows[i+1][1][-1]))\n",
        "        print()\n",
        "        del(shows[i+1])\n",
        "    else:\n",
        "        i += 1\n",
        "\n",
        "\n",
        "#Removing intervals which are overlapping between two shows.\n",
        "#Example: A - 01:00 to 10:00\n",
        "#         B - 09:00 to 12:00\n",
        "#         C - 10:00 to 20:00\n",
        "DOUBLE_OVERLAP_THRESHOLD = 0.85\n",
        "i=1\n",
        "while (i<len(shows)-1):\n",
        "    curr_show = len(shows[i][1]) #Length of current show\n",
        "    diff1 = len([x for x in shows[i][1] if x in range(shows[i-1][1][0],shows[i-1][1][-1])]) #Left side overlapping\n",
        "    overlap1 = diff1/curr_show\n",
        "    diff2 = len([x for x in shows[i][1] if x in range(shows[i+1][1][0],shows[i+1][1][-1])]) #Right side overlapping\n",
        "    overlap2 = diff2/curr_show\n",
        "    net_overlap = overlap1 + overlap2\n",
        "\n",
        "#actual algorithm\n",
        "    if(net_overlap > DOUBLE_OVERLAP_THRESHOLD):\n",
        "        print('Hosts: {} and {} and {}'.format(shows[i-1][0],shows[i][0],shows[i+1][0]))\n",
        "        print('Original durations: {} to {} and {} to {} and {} to {}'.format(sec2HMS(shows[i-1][1][0]),sec2HMS(shows[i-1][1][-1]),sec2HMS(shows[i][1][0]),sec2HMS(shows[i][1][-1]),sec2HMS(shows[i+1][1][0]),sec2HMS(shows[i+1][1][-1])))\n",
        "        #         print('Total duration: '+sec2HMS(diff))\n",
        "        #         print('Overlap: '+str(overlap))\n",
        "        print('Left overlap: {}'.format(overlap1))\n",
        "        print('Right overlap: {}'.format(overlap2))\n",
        "        print('Net overlap: {}'.format(net_overlap))\n",
        "        print()\n",
        "        del(shows[i])\n",
        "    else:\n",
        "        i+=1\n",
        "\n",
        "\n",
        "shows_refined = [shows[0]]\n",
        "prev_show = shows[0]\n",
        "for show in shows[1:]:\n",
        "    shows_refined.append([show[0], [x for x in show[1] if x>=prev_show[1][-1]] ])\n",
        "    prev_show = show\n",
        "\n",
        "\n",
        "shows = [Show(str(x[0]),x[1][0],x[1][-1]) for x in shows_refined]\n",
        "\n",
        "\n",
        "for show in shows:\n",
        "    hosts = show.hosts.split('&') #getting list of hosts of the show\n",
        "    hosts = sorted(hosts, key = lambda x: len(face_encodings[int(x)]), reverse=True) #Most occuring anchor is taken as the main anchor\n",
        "    for i in range(len(hosts)):\n",
        "        host = hosts[i]\n",
        "        host_encodings = face_encodings[int(host)]\n",
        "        # print(host_encodings)      #Getting all encodings of this host's face\n",
        "        host_prob_names = Counter(list(encoding2name(host_encodings)))\n",
        "        # print(host_prob_names) #Getting predictions of all faces\n",
        "        hosts[i] = [(x,y/len(host_encodings)) for x,y in host_prob_names.most_common()] #sorting the predictions by their frequency\n",
        "        # print(hosts[i])\n",
        "    show.hosts = hosts\n",
        "    print(show.hosts)\n",
        "#     show.hosts = sorted(hosts, key = lambda x: sum(y for _,y in x))\n",
        "# (neigh.kneighbors(face_encodings[0]))[1]\n",
        "\n",
        "\n",
        "#Extracting video metadata\n",
        "vcap = cv2.VideoCapture(vid_path)\n",
        "vid_width, vid_height = int(vcap.get(3)), int(vcap.get(4))\n",
        "vcap.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n",
        "vid_duration = int(vcap.get(cv2.CAP_PROP_POS_MSEC)/1000)\n",
        "cv2.destroyAllWindows()\n",
        "vcap.release()\n",
        "\n",
        "\n",
        "filename = os.path.splitext(os.path.basename(vid_path))[0]\n",
        "attributes = filename.split('_')\n",
        "pulldate, barcode = attributes[0], attributes[3]\n",
        "vid_txt3_path = os.path.splitext(vid_path)[0]+'.txt3'\n",
        "txt3_subtitles = None\n",
        "\n",
        "#common headers for all cuts - default values (to be used if this column is not present in the txt3)\n",
        "OVD = 'OVD|'+filename+'.mp4'  \n",
        "OID = 'OID|'\n",
        "COL = 'COL|Communication Studies Archive, UCLA'\n",
        "SRC = 'SRC|Rosenthal Collection, UCLA'\n",
        "LAN = 'LAN|ENG'\n",
        "LBT = 'LBT|'\n",
        "\n",
        "if os.path.exists(vid_txt3_path):\n",
        "    txt3_lines = open(vid_txt3_path, 'r').read().splitlines()\n",
        "    \n",
        "    for i in range(len(txt3_lines)):\n",
        "        if txt3_lines[i][3]!='|': #Header lines end\n",
        "            txt3_headers = txt3_lines[:i]\n",
        "            txt3_subtitles = txt3_lines[i:] #Subtitles' lines start here\n",
        "            break\n",
        "            \n",
        "    for header in txt3_headers:\n",
        "        if header[:3]=='TOP':\n",
        "            OVD = 'OVD|'+header[4:]\n",
        "        elif header[:3]=='UID':\n",
        "            OID = 'OID|'+header[4:]\n",
        "        elif header[:3]=='COL':\n",
        "            COL = header\n",
        "        elif header[:3]=='SRC':\n",
        "            SRC = header\n",
        "        elif header[:3]=='LAN':\n",
        "            LAN = header\n",
        "\n",
        "\n",
        "#Cutting shows from the main video + making a .txt file for each\n",
        "for n_show, show in enumerate(shows):\n",
        "    \n",
        "    channel = 'unknown-channel' #until the work with IMDb is done\n",
        "    channel = channel.replace(' ', '_')\n",
        "    \n",
        "    main_host = show.hosts[0][0]\n",
        "    if main_host[1] > 0.45: #If majority predictions are of the same person\n",
        "        host_name = main_host[0]\n",
        "    else:\n",
        "        host_name = 'unknown-host'\n",
        "#     host_name = show.hosts[0][0][0]\n",
        "    host_name = host_name.replace(' ', '_')\n",
        "    cut_filename = '_'.join((pulldate, barcode, '-'.join((str(n_show+1), str(len(shows)))), channel, host_name))\n",
        "    cut_path = os.path.join(output_path, cut_filename)\n",
        "    cut_starttime = (int(max(0, show.start_time - 60))) #using a buffer of 1 minute\n",
        "                            \n",
        "    if n_show==len(shows)-1: #last show                            \n",
        "        cut_endtime = (int(min(show.end_time + 60, vid_duration)))\n",
        "    else:\n",
        "        cut_endtime = (int(shows[n_show+1].start_time)) #till the start of next shows\n",
        "    \n",
        "    cut_duration = sec2HMS(cut_endtime - cut_starttime)\n",
        "    cut_starttime = sec2HMS(cut_starttime)\n",
        "    \n",
        "    ffmpeg_command = 'ffmpeg -ss {} -t {} -i {} -vcodec copy -acodec copy {}.mp4'.format(cut_starttime, cut_duration, vid_path, cut_path)\n",
        "    os.system(ffmpeg_command)\n",
        "    \n",
        "    TOP = 'TOP|'+cut_filename+'.mp4'\n",
        "    UID = 'UID|' #TODO: Generate UUID in the cluster\n",
        "    TTL = 'TTL|'\n",
        "    PID = 'PID|'\n",
        "    CMT = 'CMT|'\n",
        "    INF = 'INF|'\n",
        "    for i, host in enumerate(show.hosts):\n",
        "        INF += 'probable_host'+str(i+1)+':'+'_'.join([pred[0].replace(' ','-') for pred in host][:5])+'_'\n",
        "    INF = INF[:-1]        \n",
        "    DUR = 'DUR|'+cut_duration\n",
        "    TMS = 'TMS|'+cut_starttime+'-'+sec2HMS(cut_endtime)\n",
        "    VID = 'VID|{}x{}'.format(vid_width, vid_height)\n",
        "    \n",
        "    #initializing with headers\n",
        "    cut_txt_lines = [TOP, COL, UID, SRC, TTL, PID, CMT, DUR, VID, LAN, LBT, OVD, OID, TMS, INF] \n",
        "    \n",
        "    sub_starttime = pulldate.replace('-','') + cut_starttime.replace(':','')\n",
        "    sub_endtime = pulldate.replace('-','') + sec2HMS(cut_endtime).replace(':','')\n",
        "    \n",
        "    \n",
        "    if txt3_subtitles:\n",
        "        for sub_idx in range(len(txt3_subtitles)): #TODO: maybe make the starting time 0 for each?\n",
        "            curr_sub = txt3_subtitles[sub_idx]\n",
        "            \n",
        "            if curr_sub[:14] >= sub_starttime:\n",
        "                if curr_sub[:14] <= sub_endtime:\n",
        "                    cut_txt_lines.append(txt3_subtitles[sub_idx])\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "    with open(cut_path+'.txt', 'w') as f:\n",
        "        for line in cut_txt_lines:\n",
        "            f.write(\"%s\\n\" % line)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6cdda5112dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0LLSWSG1Y6k"
      },
      "source": [
        "\"\"\" After getting all professions of a person we can easily confirm that\n",
        "    our previous top 5 prediction is right or not & by using this we find \n",
        "    correct anchor.\n",
        "\"\"\"       \n",
        "\n",
        "def get_wiki_professions(Anchor_Name):\n",
        "  if len(wiki.search(Anchor_Name)) == 0:\n",
        "    return NONE\n",
        "\n",
        "  Read_Namebasics = pd.read_table(NAME_BASICS, sep='\\t')\n",
        "  Store_name = Read_Namebasics.primaryName.tolist()\n",
        "\n",
        "  try:\n",
        "    if Anchor_Name in Store_name:\n",
        "      anchor_content = (((wiki.page(Anchor_Name,auto_suggest=False)).content).replace(' and ',', ').replace('.','. '))\n",
        "    else:\n",
        "      return NONE\n",
        "  except wiki.exceptions.PageError as p:\n",
        "    if PAGE_ERROR in str(p):\n",
        "      return NONE\n",
        "  except wiki.exceptions.DisambiguationError as e:\n",
        "    Read_Namebasics = pd.read_table(NAME_BASICS, sep='\\t')\n",
        "    Store_name = Read_Namebasics.primaryName.tolist()\n",
        "    if Anchor_Name in Store_name:\n",
        "      # print(e.options)\n",
        "      list_disambiguation_error = e.options\n",
        "      for i in list_disambiguation_error:\n",
        "        if i.find(DISAMBIGUATION_ERROR)!= -1:\n",
        "          return NONE\n",
        "        # print(i)\n",
        "        anchor_content = (((wiki.page(i,auto_suggest=False)).content).replace(' and ',', ').replace('.','. '))\n",
        "        # print(anchor_content)\n",
        "        \n",
        "        split_type = SPLIT_TYPES\n",
        "\n",
        "        split_type_list = []\n",
        "        prefix = None\n",
        "        for x in split_type:\n",
        "          if x in anchor_content:\n",
        "            prefix = x\n",
        "            split_type_index = anchor_content.find(prefix)\n",
        "            #  print(index_split_type)\n",
        "            pro = split_type_list.append(split_type_index)\n",
        "          else:\n",
        "            split_type_list.append(1000)\n",
        "\n",
        "        low_index = split_type[split_type_list.index(min(split_type_list))]\n",
        "        #  print(low_index)   \n",
        "            \n",
        "\n",
        "        if prefix is None:\n",
        "          return NONE\n",
        "\n",
        "        final_split = anchor_content.split(low_index)[1].split('. ')[0].split(', ')\n",
        "        # print(final_split)\n",
        "        final_professions = [anchor_content.lower() for anchor_content in final_split]\n",
        "        # return final_professions\n",
        "        # print(final_professions)\n",
        "        required_professions = PROFESSION_TYPES\n",
        "\n",
        "        check_individual_prof = []\n",
        "        for x in required_professions:\n",
        "          final_check= [(x in y) for y in final_professions]\n",
        "          check_individual_prof.append(any(final_check))\n",
        "          # print(x)\n",
        "        \n",
        "        if (any(check_individual_prof)==True):\n",
        "          return final_professions\n",
        "      else:\n",
        "        return NONE\n",
        "    \n",
        "    else:\n",
        "      return NONE     \n",
        "\n",
        "  # anchor_content = (((wiki.page(name,auto_suggest=False)).content).replace(' and ',', ').replace('.','. '))\n",
        "  #  print(anchor_content)\n",
        "  split_type = SPLIT_TYPES\n",
        "  \n",
        "  split_type_list = []\n",
        "  prefix = None\n",
        "  for x in split_type:\n",
        "    if x in anchor_content:\n",
        "      prefix = x\n",
        "      split_type_index = anchor_content.find(prefix)\n",
        "      #  print(index_split_type)\n",
        "      pro = split_type_list.append(split_type_index)\n",
        "    else:\n",
        "      split_type_list.append(1000)\n",
        "\n",
        "  low_index = split_type[split_type_list.index(min(split_type_list))]\n",
        "  #  print(low_index)   \n",
        "      \n",
        "\n",
        "  if prefix is None:\n",
        "    return NONE\n",
        "\n",
        "  final_split = anchor_content.split(low_index)[1].split('. ')[0].split(', ')\n",
        "  # print(final_split)\n",
        "  final_professions = [anchor_content.lower() for anchor_content in final_split]\n",
        "  return final_professions\n",
        "\n",
        "\n",
        "def check_wiki_professions(list_profession):\n",
        "  required_professions = PROFESSION_TYPES\n",
        "\n",
        "  check_individual_prof = []\n",
        "  for x in required_professions:\n",
        "    final_check= [(x in y) for y in list_profession]\n",
        "    check_individual_prof.append(any(final_check))\n",
        "    # print(final_check)\n",
        "    # print(x)\n",
        "  \n",
        "  if (any(check_individual_prof)==True):\n",
        "    # print(check_individual_prof)\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "\"\"\"  From top 5 anchor we get most probable anchor in that show\n",
        "     Make sure that the input anchors list follows given format. Here's an example\n",
        "     Anchor_list = \"Colleen-Williams_Ashwini-Bhave_Elaine-Quijano_Becky-Hobbs_Heidi-Collins\"\n",
        "\"\"\"\n",
        "def get_correct_anchor(top_5):\n",
        "  top_5_list = (top_5.replace('-',' ').replace('_',',')).split(',')\n",
        "  # print(top_5_list)\n",
        "  top_5_list = list(filter(None, top_5_list))\n",
        "  for x in top_5_list:\n",
        "    # print(x)\n",
        "    # if Anchor_Name in Store_name:\n",
        "    # pqe = check_wiki_professions(get_wiki_professions(x)) is True:\n",
        "\n",
        "    if check_wiki_professions(get_wiki_professions(x)) is True:\n",
        "      Read_Namebasics = pd.read_table(NAME_BASICS, sep='\\t')\n",
        "      Store_name = Read_Namebasics.primaryName.tolist()\n",
        "      if x in Store_name:\n",
        "        return x \n",
        "\n",
        "    # if pqe is True:\n",
        "      # list_of_selected_anchor.append(x)\n",
        "     \n",
        "  return ''\n",
        "\n",
        "\n",
        "## get finals anchors from the multiple probable hosts\n",
        "\n",
        "def Final_Anchor(txt_path):\n",
        "  filter_probable_list = []\n",
        "  all_lines = []                              \n",
        "  with open (txt_path) as myfile:  \n",
        "      for line in myfile:                   \n",
        "          all_lines.append(line)\n",
        "  # print(len(all_lines))                      \n",
        "  last_line = all_lines[len(all_lines)-1].split('INF|')[1].split('\\n')[0].replace(' ','')\n",
        "  # print(last_line)\n",
        "  avoid_num = ''.join([i for i in last_line if not i.isdigit()])\n",
        "  probable_host_list = avoid_num.split('probable_host:')\n",
        "  probable_host_list = list(filter(None, probable_host_list))\n",
        "\n",
        "  # print(probable_host_list)\n",
        "\n",
        "  for i in probable_host_list:\n",
        "    filter_probable_list.append(get_correct_anchor(i))\n",
        "    filter_probable_list = list(filter(None, filter_probable_list))\n",
        "\n",
        "  confident_host_line = 'Confident Host: '\n",
        "  for i in filter_probable_list:\n",
        "    confident_host_line += str(i) +' |';\n",
        "\n",
        "  with open(txt_path, \"a+\") as file_object:\n",
        "    file_object.seek(0)\n",
        "    data = file_object.read(100)\n",
        "    if len(data) > 0 :\n",
        "        file_object.write(\"\\n\")\n",
        "    file_object.write(confident_host_line)  \n",
        "  return filter_probable_list\n",
        "\n",
        "\n",
        "#here we are getting all anchors present in a video\n",
        "def get_list_anchors(folder_path):#folder path = where all txt files placed of a show \n",
        "  list_of_anchors = []\n",
        "  list_txt_files = os.listdir(folder_path)\n",
        "  for i in list_txt_files:\n",
        "    txt_file_path = folder_path+'/'+i\n",
        "    # print(Final_Anchor(txt_file_path))\n",
        "    list_of_anchors.append(Final_Anchor(txt_file_path))\n",
        "    print(\"Wait it will take 10-15 min\")\n",
        "  return list_of_anchors\n",
        "\n",
        "\n",
        "#this will give us network name either from anchor or show\n",
        "def get_channel(name):\n",
        "  try:\n",
        "    anchor_content = ((((wiki.page(name,auto_suggest=False)).content).replace(' and ',', ').replace('.','. ')))\n",
        "  \n",
        "  except wiki.exceptions.PageError as p:\n",
        "    if PAGE_ERROR in str(p):\n",
        "      return NONE\n",
        "  \n",
        "  except wiki.exceptions.DisambiguationError as e:\n",
        "    list_disambiguation_error = e.options\n",
        "    for anchor_name in list_disambiguation_error:\n",
        "      if anchor_name.find(DISAMBIGUATION_ERROR)!= -1:\n",
        "        return NONE\n",
        "      \n",
        "      else:\n",
        "        anchor_content = ((((wiki.page(anchor_name,auto_suggest=False)).content).replace(' and ',', ').replace('.','. ')))\n",
        "  # print(anchor_content)\n",
        "  Networks_list = NETWORK_TYPES\n",
        "  Network_filter = []\n",
        "    \n",
        "  for network in Networks_list:\n",
        "    # print(word)\n",
        "    if anchor_content.find(network)!=-1:\n",
        "      Network_filter.append(network)\n",
        "  return Network_filter\n",
        "\n",
        "\n",
        "def get_majority_network(anchors_list):\n",
        "  list_of_network = []\n",
        "  for anchorlist in anchors_list:\n",
        "    for anchor in anchorlist:\n",
        "      # print(a)\n",
        "      list_of_network.append(get_channel(anchor))\n",
        "  list_of_network = list(filter(None,list_of_network))\n",
        "  print(list_of_network)\n",
        "\n",
        "  X ={} #dict contains networks and respected votes \n",
        "  for network in list_of_network:\n",
        "    for particular in network:\n",
        "      if particular in X:\n",
        "        X[particular] +=1\n",
        "      else:\n",
        "        X[particular] = 1\n",
        "  print(X)   \n",
        "  Max_votes = max(X.values())\n",
        "  # print(Max_votes)\n",
        "  Majority_Network = [particular for particular, votes in X.items() if votes == Max_votes]\n",
        "  return Majority_Network\n",
        "\n",
        "\n",
        "def get_nconst(Anchor_Name,Pull_Year):\n",
        "\n",
        "  Read_Titlebasics = pd.read_table(TITLE_BASICS, sep='\\t')\n",
        "  Read_Namebasics = pd.read_table(NAME_BASICS, sep='\\t')\n",
        "  \n",
        "  Read_Namebasics.primaryProfession = Read_Namebasics.primaryProfession.fillna(\"Null\")\n",
        "  filter_year = Read_Namebasics[Read_Namebasics.deathYear >= Pull_Year]\n",
        "  Search_name = filter_year[filter_year.primaryName == str(Anchor_Name)]\n",
        "\n",
        "  if len(Search_name.index) == 1:\n",
        "    for index, row in Search_name.iterrows():\n",
        "      prepare_list_kft = row.knownForTitles.split(',')\n",
        "      return [row.nconst,prepare_list_kft]\n",
        "\n",
        "  else:\n",
        "     \n",
        "    titleakas = pd.read_table(TITLE_AKAS, sep='\\t')\n",
        "    titleakas.region = titleakas.region.fillna(REGION_TYPE)\n",
        "    filter_region = titleakas[titleakas.region == REGION_TYPE]\n",
        "    filter_region_titleid = filter_region.titleId.tolist()\n",
        "\n",
        "    for index, row in Search_name.iterrows():\n",
        "      prepare_list_kft = row.knownForTitles.split(',')\n",
        "      for i in prepare_list_kft:\n",
        "        take_title = Read_Titlebasics[Read_Titlebasics.tconst == i]\n",
        "        if i in filter_region_titleid:\n",
        "          check_genres = take_title[take_title.genres.str.contains(GENRE_TYPES)]\n",
        "          if len(check_genres.index) == 1:\n",
        "            return [row.nconst,prepare_list_kft]\n",
        "    return [NONE]\n",
        "\n",
        "\n",
        "\n",
        "def get_all_shows(nConst):\n",
        "  chunksize = 6*10**6\n",
        "  titleprincipal = pd.read_table(TITLE_PRINCIPALS, chunksize = chunksize)\n",
        "  all_shows = pd.DataFrame(columns=['tconst', 'ordering', 'nconst', 'category', 'job', 'characters'])\n",
        "  #Initializing with the same columns as title.principals\n",
        "  for i,chunk in enumerate((titleprincipal)):\n",
        "    req_shows = chunk[chunk['nconst'] == nConst]\n",
        "    print(' {} shows in chunk {}'.format(len(req_shows), i))\n",
        "    all_shows = all_shows.append(req_shows, ignore_index = True)\n",
        "    # filter_category = all_shows[all_shows.category == 'self']\n",
        "\n",
        "  return all_shows.tconst.tolist()\n",
        "\n",
        "\n",
        "# get final titles afters union or filter\n",
        "def get_show_names(Anchor_Name,Pull_Year):\n",
        "  store_US_titles = []\n",
        "\n",
        "  store_nconst = get_nconst(Anchor_Name,Pull_Year)\n",
        "\n",
        "  if store_nconst[0] == NONE:\n",
        "    return None\n",
        "\n",
        "  titlecrew = pd.read_table('/content/drive/MyDrive/Datasets/IMDB_Datasets/title.crew.tsv/data.tsv' ,sep='\\t')\n",
        "  nconst_in_dir = titlecrew[titlecrew.directors.str.contains(store_nconst[0])]\n",
        "  nconst_in_writer = titlecrew[titlecrew.writers.str.contains(store_nconst[0])]\n",
        "  tconst_in_crew = list(set().union(nconst_in_dir.tconst.tolist(),nconst_in_writer.tconst.tolist()))\n",
        "\n",
        "  # Combine_list_1 = list(set().union(store_nconst[1],get_all_shows(store_nconst[0])))\n",
        "  Combine_list_2 = set().union(store_nconst[1],get_all_shows(store_nconst[0]),tconst_in_crew)\n",
        "\n",
        "# Here applying US(region) filter in the shows\n",
        "\n",
        "  titleakas = pd.read_table(TITLE_AKAS, sep='\\t')\n",
        "  titleakas.region = titleakas.region.fillna(REGION_TYPE)\n",
        "  filter_region = titleakas[titleakas.region == REGION_TYPE]\n",
        "  filter_region_titleid = filter_region.titleId.tolist()\n",
        "  for x in Combine_list_2:\n",
        "    if x in filter_region_titleid:\n",
        "      store_US_titles.append(x)\n",
        "\n",
        "  # return store_US_titles\n",
        "  tConsts= set(store_US_titles)\n",
        "\n",
        "  chunksize = 6*10**6\n",
        "\n",
        "  titlebasics = pd.read_table(TITLE_BASICS, chunksize = chunksize)\n",
        "  Show_Names = pd.DataFrame(columns=['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres'])\n",
        "  for i,chunk in enumerate(titlebasics):\n",
        "    show_names = chunk[chunk['tconst'].isin(tConsts)]\n",
        "    print('{} shows in chunk {}'.format(len(show_names), i))\n",
        "    Show_Names = Show_Names.append(show_names, ignore_index = True)\n",
        "    # print(Show_Names)\n",
        "    filter_title_type = Show_Names[Show_Names['titleType']==TITLE_TYPE]\n",
        "    filter_genres = filter_title_type[filter_title_type.genres.str.contains(GENRE_TYPES)]\n",
        "    filter_end_year = filter_genres[filter_genres.endYear >= Pull_Year]\n",
        "    filter_start_year = filter_end_year[filter_end_year.startYear.apply(str) <=Pull_Year]\n",
        "  return filter_start_year\n",
        "\n",
        "\n",
        "def final_show(anchor_list, Pull_Year):\n",
        "  X = {} #dict contains show name and their votes\n",
        "  for host in anchor_list:\n",
        "    print(host)\n",
        "    store_show_names = get_show_names(host,Pull_Year)\n",
        "    if store_show_names is None:\n",
        "      title = SHOW_NOT_IN_IMDB\n",
        "      if title in X:\n",
        "        X[title] +=1\n",
        "      else:\n",
        "        X[title] = 1\n",
        "    \n",
        "    else:\n",
        "      for title in store_show_names.primaryTitle.tolist():\n",
        "        if title in X:\n",
        "          X[title] +=1\n",
        "        else:\n",
        "          X[title] = 1\n",
        "  if bool(X) is False:\n",
        "    return NONE\n",
        "  else:\n",
        "    Max_votes = max(X.values())\n",
        "    # print(Max_votes)\n",
        "    Majority_Shows = [title for title, votes in X.items() if votes==Max_votes]\n",
        "    if len(Majority_Shows) is 1:\n",
        "      print(Majority_Shows)\n",
        "      # return Majority_Shows\n",
        "    else:\n",
        "      Open_Subtitles = open(\"/content/drive/MyDrive/Datasets/Subtitles/2006-01-04_0000_US_00000063_V2_MB7_VHS7_H11_MS.txt3\")\n",
        "      Read_Subtitles = Open_Subtitles.read()\n",
        "\n",
        "      for show in Majority_Shows:\n",
        "        if Read_Subtitles.find(show.upper())!=-1:\n",
        "          print(show)\n",
        "          return show\n",
        "          \n",
        "\n",
        "      for show in Majority_Shows:\n",
        "        for network in get_majority_network(anchor_list):\n",
        "          if show.find(network)!=-1:\n",
        "            print(show)\n",
        "            return show\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqqFzQnBDMVR",
        "outputId": "e09314ae-5bbc-4d6f-c7b2-29063d7f4474"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhRxKrACCpDW",
        "outputId": "744af76a-4430-45cf-bccf-624eb091f0a7"
      },
      "source": [
        "final_show(get_list_anchors(\"/content/drive/MyDrive/Datasets/newtest\"),\"2006\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait it will take 10-15 min\n",
            "Wait it will take 10-15 min\n",
            "['Rosemary Church', 'Boy Abunda']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:339: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:339: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Kyra Phillips', 'Reggie Yates']\n",
            "['Show is not present in IMDb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WmbCwJzDg8w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}